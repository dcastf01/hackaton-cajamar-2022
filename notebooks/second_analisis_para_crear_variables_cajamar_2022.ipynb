{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmMVaxH_GPp3"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHCpDwBKRrcZ"
      },
      "outputs": [],
      "source": [
        "# !pip install xgboost catboost tsfresh seaborn\n",
        "# !pip install pycaret[full]\n",
        "# !pip  matplotlib==3.1.3\n",
        "# !pip install pycaret-ts-alpha\n",
        "# !pip install scipy>=1.5\n",
        "# !pip install pystan==2.19.1.1\n",
        "# !pip install Cython\n",
        "# !pip install prophet\n",
        "# !pip install -U kaleido\n",
        "# !pip install numpy --upgrade\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Hackaton cajamar 2022/Cajamar_Water_Footprint.zip\" \"/content/data.zip\"\n",
        "!unzip -P  UH2022@CwF data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soUcrUgdGBLg"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjSaR_21Rz_G"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "import pycaret\n",
        "import pandas as pd\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from prophet import Prophet\n",
        "\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "#https://towardsdatascience.com/time-series-feature-extraction-on-really-large-data-samples-b732f805ba0e\n",
        "from tsfresh import extract_features, extract_relevant_features, select_features\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfisF7GxyvGD"
      },
      "outputs": [],
      "source": [
        "target='total'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQBrgErl22ny"
      },
      "outputs": [],
      "source": [
        "def negative_value_to_0(df):\n",
        "        df['DELTAINTEGER']=np.where(df['DELTAINTEGER']<0,0,df['DELTAINTEGER'])\n",
        "        return df\n",
        "def negative_value_to_mean(df):\n",
        "        df['DELTAINTEGER']=np.where(df['DELTAINTEGER']<0,df['DELTAINTEGER'].mean(),df['DELTAINTEGER'])\n",
        "        return df    \n",
        "def check_if_we_have_negative(df,column='DELTAINTEGER'):\n",
        "    return df[df[column] < 0][column].any()\n",
        "def clean_data(df):\n",
        "    def clean_instances_with_minor_error(df,special_unique_negative_id):\n",
        "            \n",
        "        negative_instances=df[df['DELTAINTEGER']<0].sort_values(['ID','SAMPLETIME'])\n",
        "        normal_unique_negative=list(negative_instances[~negative_instances.isin(special_unique_negative_id)]['ID'].unique())\n",
        "        # print(df[df['ID'].isin(normal_unique_negative)].describe())\n",
        "        df_normal_unique_negative=pd.DataFrame()\n",
        "        for id_unique in tqdm(normal_unique_negative):\n",
        "            df_per_id=df[df['ID']==id_unique]\n",
        "            df_per_id=negative_value_to_0(df_per_id)\n",
        "            df_per_id[\"ID\"]\n",
        "            df_normal_unique_negative=pd.concat([df_normal_unique_negative,df_per_id])#.reset_index()\n",
        "        # print(df_normal_unique_negative.describe())\n",
        "        return df_normal_unique_negative\n",
        "\n",
        "    def clean_outliers_with_diverse_strategy(df,id_outliers_with_a_fews_peaks:dict):\n",
        "        \n",
        "        df_outlier_unique_negative=pd.DataFrame()\n",
        "        for id_unique,args in tqdm(id_outliers_with_a_fews_peaks.items()):\n",
        "            df_outlier_strategy_per_id=df[df['ID']==id_unique].sort_values(['ID','SAMPLETIME'])   \n",
        "            # print(df_outlier_strategy_per_id.describe())     \n",
        "            df_outlier_strategy_per_id['DELTAINTEGER']=np.where(   \n",
        "                    df_outlier_strategy_per_id['DELTAINTEGER']<0,\n",
        "                    df_outlier_strategy_per_id[(df_outlier_strategy_per_id['DELTAINTEGER']>0)&(df_outlier_strategy_per_id['DELTAINTEGER']<args['limit_max'])]['DELTAINTEGER'].mean(),\n",
        "                    np.where(\n",
        "                        df_outlier_strategy_per_id['DELTAINTEGER']>args['limit_max'],\n",
        "                            args['limit_max']+0.2,\n",
        "                            df_outlier_strategy_per_id['DELTAINTEGER']\n",
        "                    )\n",
        "                    ) \n",
        "            # print(df_outlier_strategy_per_id.describe()) \n",
        "            exist_negative=check_if_we_have_negative(df_outlier_strategy_per_id)\n",
        "            if exist_negative:\n",
        "                df_outlier_strategy_per_id=negative_value_to_mean(df_outlier_strategy_per_id)\n",
        "            df_outlier_unique_negative=pd.concat([df_outlier_unique_negative,df_outlier_strategy_per_id])\n",
        "        # print(df_outlier_unique_negative.describe())\n",
        "        return df_outlier_unique_negative\n",
        "\n",
        "    def strategy_inverse_to_clean_data(df,id_to_inverse_negative_values:list):\n",
        "\n",
        "        df_inverse_strategy_unique_negative=pd.DataFrame()\n",
        "        for id_unique in tqdm(id_to_inverse_negative_values):\n",
        "\n",
        "            df_inverse_strategy_per_id=df[df['ID']==id_unique].sort_values(['ID','SAMPLETIME'])\n",
        "            df_inverse_strategy_per_id['DELTAINTEGER']=abs(df_inverse_strategy_per_id['DELTAINTEGER'])\n",
        "\n",
        "            exist_negative=check_if_we_have_negative(df_inverse_strategy_per_id)\n",
        "            if exist_negative:\n",
        "                print('ID ',id_unique,'already with negatives')\n",
        "                df_inverse_strategy_per_id=negative_value_to_mean(df_inverse_strategy_per_id)\n",
        "            df_inverse_strategy_unique_negative=pd.concat([df_inverse_strategy_unique_negative,df_inverse_strategy_per_id])\n",
        "        return df_inverse_strategy_unique_negative\n",
        "\n",
        "    special_unique_negative_id=[57, 301,374,379,493,635,845,\n",
        "                            873, 907,1218,1280,1468,1487,\n",
        "                            1506,1739,1873,1884, 2063,2121,\n",
        "                            2711\n",
        "                            ] #obtenidos del estudio exploratorio\n",
        "    id_to_inverse_negative_values=[\n",
        "                                    2121,\n",
        "                                    2711,\n",
        "                                    1873,\n",
        "                                    379,\n",
        "                                    1739,\n",
        "                                    1468,]\n",
        "    id_outliers_with_a_fews_peaks={\n",
        "        57:{'limit_max':100},\n",
        "        374:{'limit_max':2700},\n",
        "        493:{'limit_max':200},\n",
        "        635:{'limit_max':200},\n",
        "        873:{'limit_max':200},\n",
        "        907:{'limit_max':50},\n",
        "        1218:{'limit_max':400},\n",
        "        1280:{'limit_max':250},\n",
        "        2063:{'limit_max':300},\n",
        "        1884:{'limit_max':150},\n",
        "        1506:{'limit_max':200},\n",
        "        1487: { 'limit_max':150},\n",
        "        301:{ 'limit_max':150},\n",
        "        # cuando se pase esto a dias poner la media en ese fragmento que desaparece de este ID el id 845:\n",
        "        #valores entre el 1 de mayo y el 8 de junio de 2019 se tienen que corregir de forma diaria\n",
        "        845:{ 'limit_max':250},    \n",
        "        }\n",
        "    ids_with_negatives=negative_instances=df[df['DELTAINTEGER']<0].sort_values(['ID','SAMPLETIME'])['ID'].unique()\n",
        "    df_with_negatives=df[df[\"ID\"].isin(ids_with_negatives)].copy()\n",
        "    df_without_negatives=df[~df[\"ID\"].isin(ids_with_negatives)].copy() #pendiente ver los picos máximos para solucionarlo\n",
        "    df_normal_unique_negative=clean_instances_with_minor_error(df_with_negatives,special_unique_negative_id)\n",
        "    df_outlier_unique_negative=clean_outliers_with_diverse_strategy(df_with_negatives,id_outliers_with_a_fews_peaks)\n",
        "    df_inverse_strategy_unique_negative=strategy_inverse_to_clean_data(df_with_negatives,id_to_inverse_negative_values)\n",
        "    df_total=pd.concat([df_without_negatives,df_normal_unique_negative,df_outlier_unique_negative,df_inverse_strategy_unique_negative])\n",
        "    \n",
        "    return df_total.sort_values(['ID','SAMPLETIME'])\n",
        "\n",
        "def outliers_after_groupby_period(df,outliers_fraction=0.01):\n",
        "    method_forest=IsolationForest(contamination=outliers_fraction)\n",
        "    df_after_outliers=pd.DataFrame()\n",
        "    for id_unique in tqdm(df.ID.unique()):\n",
        "        df_per_id=df[df['ID']==id_unique].copy()\n",
        "        yhat = method_forest.fit_predict(df_per_id['total'].to_numpy().reshape(-1,1))\n",
        "        mask = yhat != -1\n",
        "        df_per_id['total']=np.where((mask) | (df_per_id.index> '2020-01-22'),\n",
        "                                    df_per_id['total'],\n",
        "                                    df_per_id['total'].shift(periods=7).fillna(method='bfill')\n",
        "                            )\n",
        "        df_after_outliers=pd.concat([df_after_outliers,df_per_id])\n",
        "    return df_after_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALXMaNw75luV"
      },
      "outputs": [],
      "source": [
        "def get_range_to_predic(start='2020-01-31',last_date='2020-01-31',periods=1,):\n",
        "       \n",
        "        freq='D'\n",
        "        dates = pd.date_range(\n",
        "            start=start,\n",
        "            periods=periods,  # An extra in case we include start\n",
        "            freq=freq)\n",
        "    \n",
        "        return dates\n",
        "def add_new_date_per_id(df_per_id_but_without_column_id,date,id_unique):\n",
        "    df_to_add_new_date=pd.DataFrame()\n",
        "   \n",
        "    df_to_add_new_date.index=get_range_to_predic(start=date,last_date=date)\n",
        "    df_to_add_new_date.index = df_to_add_new_date.index.set_names(['date'])\n",
        "    df_to_add_new_date.reset_index(inplace=True)\n",
        "    \n",
        "    df_to_add_new_date=df_to_add_new_date[df_to_add_new_date['date']==date]\n",
        "      \n",
        "    return df_to_add_new_date\n",
        "\n",
        "def get_raw_data():\n",
        "        data_path='/content/Modelar_UH2022.txt'\n",
        "        df=pd.read_csv( data_path,sep=\"|\")\n",
        "        df=df[df['ID']<=25]\n",
        "        # df=df[df['ID'].isin(list_of_id_with_tiny_data)]\n",
        "        logging.info(df.shape)\n",
        "        df.drop(columns=['READINGINTEGER','READINGTHOUSANDTH'],inplace=True)\n",
        "        df.fillna(0,inplace=True) #check in future\n",
        "        logging.info(df.shape)\n",
        "        df.loc[:,'SAMPLETIME']=pd.to_datetime(df['SAMPLETIME'],errors='raise',format='%Y-%m-%d')\n",
        "        return df\n",
        "\n",
        "def get_data():\n",
        "    \n",
        "    df_raw=get_raw_data()\n",
        "    df=clean_data(df_raw)\n",
        "    # df['target']\n",
        "    print(df.columns)\n",
        "    df.rename(columns={'SAMPLETIME':'date'},inplace=True)\n",
        "    df['total']=df['DELTAINTEGER']+df['DELTATHOUSANDTH']\n",
        "    df.drop(['DELTAINTEGER','DELTATHOUSANDTH'],axis=1,inplace=True)\n",
        "    print(df.columns)\n",
        "\n",
        "    df_total=pd.DataFrame()\n",
        "    for id_unique in tqdm(df.ID.unique(),desc='creating all the dates per ID',mininterval=50):\n",
        "        df_per_id=df[df['ID']==id_unique]\n",
        "        df_per_id_but_without_column_id=df_per_id.drop('ID',axis=1)\n",
        "        list_of_df_with_new_dates=[]\n",
        "        date='2020-01-31'\n",
        "        if not df_per_id.date.isin([date]).any():\n",
        "            # print('adding',date)\n",
        "            df_with_last_date=add_new_date_per_id(df_per_id_but_without_column_id,date,id_unique)\n",
        "            list_of_df_with_new_dates.append(df_with_last_date)\n",
        "        # else:print(id_unique,'ya tiene la fecha',date)\n",
        "        date='2019-02-01'\n",
        "        if not df_per_id.date.isin([date]).any():\n",
        "            # print('adding',date)\n",
        "            df_with_start_date=add_new_date_per_id(df_per_id_but_without_column_id,date,id_unique)\n",
        "            list_of_df_with_new_dates.append(df_with_start_date)\n",
        "        # else:print(id_unique,'ya tiene la fecha',date)\n",
        "        if list_of_df_with_new_dates:\n",
        "            # print('list_of_df_with_new_dates')\n",
        "            df_aux=pd.concat([df_per_id_but_without_column_id,*list_of_df_with_new_dates])\n",
        "        else:    \n",
        "            df_aux=df_per_id_but_without_column_id\n",
        "            # print('no_new_date')\n",
        "        # df_total.drop('ID',inplace=True,axis=1)\n",
        "        df_aux=df_aux.resample('D',on='date').sum()\n",
        "        df_aux['ID']=id_unique\n",
        "        df_total=pd.concat([df_total,df_aux])#.reset_index()\n",
        "\n",
        "    df_total_without_outliers=outliers_after_groupby_period(df_total)\n",
        "    # pivot_by_dates=df_total_to_pivot.pivot(\n",
        "    # index='date',\n",
        "    # columns='ID',\n",
        "    # values='total'\n",
        "    # )\n",
        "    return df_total_without_outliers\n",
        "\n",
        "def plot_series(time, series,i, format=\"-\", start=0, end=None):\n",
        "\n",
        "    \n",
        "    \n",
        "    plt.plot(time[start:end], series[start:end], format,label=i)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales (Water)\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG5IfTSfMYcg"
      },
      "outputs": [],
      "source": [
        "df=get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "34OX4HLe4_k4"
      },
      "outputs": [],
      "source": [
        "df[df['ID']==3].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QSrHsSG1BA8T"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dBbADERlBAdt"
      },
      "outputs": [],
      "source": [
        "df_total_to_pivot=df.reset_index()\n",
        "pivot_by_dates=df_total_to_pivot.pivot(\n",
        "    index='date',\n",
        "    columns='ID',\n",
        "    values='total'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0Dn2JHaBGq6"
      },
      "outputs": [],
      "source": [
        "pivot_by_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lILmP8couyjv"
      },
      "source": [
        "mirar los casos positivos con esto\n",
        "https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQHd7m6R-W5p"
      },
      "outputs": [],
      "source": [
        "#@title Researching\n",
        "is_researching = True #@param {type:\"boolean\"}\n",
        "\n",
        "id_researching=range(0,20)\n",
        "\n",
        "#no modificar los días después del 22 de enero\n",
        "\n",
        "\n",
        "\n",
        "# df_to_study=df[(df['DELTAINTEGER']>0)]['DELTAINTEGER'].quantile(0.75)+df_to_study_per_id[df_to_study_per_id['DELTAINTEGER']>0]['DELTAINTEGER'].std()*5\n",
        "# for id_unique in tqdm(df.ID.unique()):\n",
        "#     df_to_study=df_clean[df_clean['ID']==id_unique]\n",
        "#     df_to_study=df_to_study[df_to_study['DELTAINTEGER']>(df_to_study['DELTAINTEGER'].quantile(0.75)+df_to_study['DELTAINTEGER'].std()*5)]\n",
        "\n",
        "#     if not df_to_study.empty:\n",
        "    \n",
        "#         df_to_study=df[df['ID']==id_unique].sort_values(['ID','SAMPLETIME'])\n",
        "#         print('ID',id_unique,'max',df_to_study['DELTAINTEGER'].max(),'min',df_to_study['DELTAINTEGER'].min())\n",
        "#         p\n",
        "#         min=df_to_study['DELTAINTEGER'].max()\n",
        "#         print(df_to_study.describe())\n",
        "#         # plt.ylim(min-5,abs(min)+5)\n",
        "#         plt.show()\n",
        "if is_researching:\n",
        "    outliers_fraction=0.01\n",
        "    method_forest=IsolationForest(contamination=outliers_fraction)\n",
        "    df_after_outliers=pd.DataFrame()\n",
        "    plt.figure(figsize=(20,5))\n",
        "    for i,id_unique in enumerate(id_researching):\n",
        "        df_per_id=df[df['ID']==id_unique].copy()\n",
        "        \n",
        "        df_per_id.head(20)\n",
        "        yhat = method_forest.fit_predict(df_per_id['total'].to_numpy().reshape(-1,1))\n",
        "        mask = yhat != -1\n",
        "        plot_series(df_per_id.index,df_per_id['total'],f'todo {id_unique}')\n",
        "        plot_series(df_per_id[mask].index,df_per_id[mask]['total'],f'true {id_unique}')\n",
        "        plot_series(df_per_id[~mask].index,df_per_id[~mask]['total'],f'mask {id_unique}')\n",
        "        plt.title('original')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,5))\n",
        "        print('\\n')\n",
        "        df_per_id['total']=np.where((mask) | (df_per_id.index> '2020-01-22'),df_per_id['total'],\n",
        "                df_per_id['total'].shift(periods=7).fillna(method='bfill')\n",
        "                        \n",
        "                        )\n",
        "        plot_series(df_per_id.index,df_per_id['total'],f'todo {id_unique}')\n",
        "        plot_series(df_per_id[mask].index,df_per_id[mask]['total'],f'true {id_unique}')\n",
        "        plot_series(df_per_id[~mask].index,df_per_id[~mask]['total'],f'mask {id_unique}')\n",
        "        plt.title('modified')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,5))\n",
        "        df_after_outliers=pd.concat([df_after_outliers,df_per_id])\n",
        "        print('\\n'*3)\n",
        "        if i>20:\n",
        "            break\n",
        "print(df_per_id[~mask])\n",
        "    \n",
        "# df_aux.reset_index(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt3DI3PEPLZ8"
      },
      "source": [
        "##casos negativos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar1RoAKCsVX5"
      },
      "source": [
        "al agruparlo por día desaparecera el valor erroneo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEoVygVL4qR8"
      },
      "outputs": [],
      "source": [
        "#@title comprobando que los negativos están arreglados\n",
        "\n",
        "need_check_the_negatives_values = False #@param {type:\"boolean\"}\n",
        "if need_check_the_negatives_values:\n",
        "    df=get_raw_data()\n",
        "    df_clean=clean_data(df)\n",
        "    id_per_plot = 1 #@param {type:\"number\"}\n",
        "    plt.figure(figsize=(20,5))\n",
        "    special_unique_negative_id=[57, 301,374,379,493,635,845,\n",
        "                                873, 907,1218,1280,1468,1487,\n",
        "                                1506,1739,1873,1884, 2063,2121,\n",
        "                                2711\n",
        "                                ]\n",
        "    for i, id_unique in tqdm(enumerate(special_unique_negative_id),desc=\"creating plots\"):\n",
        "        \n",
        "        df_aux=df_clean[df_clean['ID'] == id_unique].sort_values(['ID','SAMPLETIME'])\n",
        "        # bin=df_aux['group_by_mean_in_total'].unique()[0]\n",
        "        plot_series(df_aux['SAMPLETIME'],df_aux['DELTAINTEGER'],id_unique)\n",
        "        # min=df_aux['DELTAINTEGER'].min()\n",
        "        if i %id_per_plot==0:\n",
        "            # plt.ylim(min-5,abs(min)+5)\n",
        "            plt.title(f'This id is {str(id_unique)}')\n",
        "            plt.axhline(y = 0, color = 'k', linestyle = '-')\n",
        "            plt.show()\n",
        "            plt.figure(figsize=(20,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0km39dPFD5Im"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "#drawing one figure with title and 3 axes below each other. Size and resolution are specified\n",
        "f, axes = plt.subplots(4,1,figsize=(18,12),dpi=600,sharex=True)\n",
        "plt.suptitle('Comparison of unscaled features at different scales',fontsize=22);\n",
        "\n",
        "#drawing boxplots of three different scales, each to separate axis\n",
        "max_id=100\n",
        "sns.boxplot(data=pivot_by_dates.iloc[:,1:max_id], ax=axes[0])\n",
        "sns.boxplot(data=pivot_by_dates.iloc[:,1:max_id], ax=axes[1]).set(ylim=(0,5000))\n",
        "sns.boxplot(data=pivot_by_dates.iloc[:,1:max_id], ax=axes[2]).set(ylim=(0,2500))\n",
        "sns.boxplot(data=pivot_by_dates.iloc[:,1:max_id], ax=axes[3]).set(ylim=(0,1000))\n",
        "\n",
        "#rotating ticks of the shared x axis by 90 degrees. The shared x axis is located on axes[2]\n",
        "for tick in axes[3].get_xticklabels():\n",
        "        tick.set_rotation(90);\n",
        "\n",
        "#setting y axis labels for each axis\n",
        "for a in axes:\n",
        "    a.set_ylabel('Unscaled values');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYP4LptQGhKF"
      },
      "source": [
        "Con estos primeros 100 ID se puede observar que el rango de cada ID es muy variado, , desde 10000 de media algunos hasta muchos entre valores comprendidos menores a 1000. Se recomienda analizar el resto de ID de una forma más descriptiva, sin gráfica ya que esto solo representa el 5% aproximadamente de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAgxCxhUHWqs"
      },
      "outputs": [],
      "source": [
        "describe_by_dates=pivot_by_dates.describe()\n",
        "describe_by_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mie9-5Y9awJZ"
      },
      "outputs": [],
      "source": [
        "describe_by_dates.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fEbyyA0BwTa"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "#drawing one figure with title and 3 axes below each other. Size and resolution are specified\n",
        "f, axes = plt.subplots(4,1,figsize=(18,12),dpi=600,sharex=True)\n",
        "plt.suptitle('Comparison of unscaled features at different scales',fontsize=22);\n",
        "\n",
        "#drawing boxplots of three different scales, each to separate axis\n",
        "max_id=100\n",
        "sns.boxplot(data=describe_by_dates.T, ax=axes[0]).set(ylim=(-5000,20000))\n",
        "sns.boxplot(data=describe_by_dates.T, ax=axes[1]).set(ylim=(0,5000))\n",
        "sns.boxplot(data=describe_by_dates.T, ax=axes[2]).set(ylim=(0,2500))\n",
        "sns.boxplot(data=describe_by_dates.T, ax=axes[3]).set(ylim=(0,1000))\n",
        "\n",
        "#rotating ticks of the shared x axis by 90 degrees. The shared x axis is located on axes[2]\n",
        "for tick in axes[3].get_xticklabels():\n",
        "        tick.set_rotation(90);\n",
        "\n",
        "#setting y axis labels for each axis\n",
        "for a in axes:\n",
        "    a.set_ylabel('Unscaled values');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMOvgU5ibsqH"
      },
      "source": [
        "De esta forma se observa que la mayoría de los id tienen valores similares, con un rango intercuartilico de una amplitud de 500 en la media y desviación típica. Igual que el valor máximo tiene una gran amplitud de valores posibles y que el mínimo de normal es 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kh2OhNyc7cm"
      },
      "outputs": [],
      "source": [
        "plot_series(describe_by_id.T.index,describe_by_id.T['mean'],'mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdx5BNjZfbe8"
      },
      "source": [
        "##Creating groups of ID per mean and per std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm-sMevIf3GH"
      },
      "source": [
        "Empezamos creando dos grupos de ID iniciales, el primero de ellos mayor a 2000 y el segundo entre 1000 a 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjt48zsVvPIU"
      },
      "outputs": [],
      "source": [
        "describe_by_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksNMoJ_vMT6k"
      },
      "outputs": [],
      "source": [
        "group_2000=describe_by_dates.T[describe_by_dates.T['mean']>2000]\n",
        "group_1000_2000=describe_by_dates.T[(describe_by_dates.T['mean']>1000)&(describe_by_dates.T['mean']<2000)]\n",
        "group_0=describe_by_dates.T[describe_by_dates.T['mean']==0]\n",
        "rest_df=describe_by_dates.T[(describe_by_dates.T['mean']<=1000)&(describe_by_dates.T['mean']>0)]\n",
        "print(group_2000.shape)\n",
        "print(group_1000_2000.shape)\n",
        "print(group_0.shape)\n",
        "rest_df['group_by_mean_in_total']=pd.qcut(rest_df['mean'],q=50,labels=range(2,52))\n",
        "rest_df.groupby('group_by_mean_in_total').count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rYhvGGYw6I"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(\n",
        "    rest_df['std'], norm_hist=False, kde=False, bins=50, hist_kws={\"alpha\": 1}\n",
        ").set(xlabel='Sale Price', ylabel='Count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgk15hwRg0V6"
      },
      "outputs": [],
      "source": [
        "group_2000=describe_by_dates.T[describe_by_dates.T['std']>2000]\n",
        "group_1000_2000=describe_by_dates.T[(describe_by_dates.T['std']>1000)&(describe_by_dates.T['std']<2000)]\n",
        "group_0=describe_by_dates.T[describe_by_dates.T['std']==0]\n",
        "rest_df=describe_by_dates.T[(describe_by_dates.T['std']<=1000)&(describe_by_dates.T['std']>0)]\n",
        "print(group_2000.shape)\n",
        "print(group_1000_2000.shape)\n",
        "print(group_0.shape)\n",
        "rest_df['group_by_std_in_total']=pd.qcut(rest_df['std'],q=50,labels=range(2,52))\n",
        "rest_df.groupby('group_by_std_in_total').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzdUTWh6ONx9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(\n",
        "    rest_df['std'], norm_hist=False, kde=False, bins=50, hist_kws={\"alpha\": 1}\n",
        ").set(xlabel='Sale Price', ylabel='Count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM7O9mmbOxlv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaWA2-kJdvaB"
      },
      "source": [
        "Ya que los ID tienen tanta variabilidad entre ellos se ha decidido agrupar por grupos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXlGN0N9d3X4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1smiai0cfGl5"
      },
      "outputs": [],
      "source": [
        "def create_cluster_by_mean(df,kind):\n",
        "    #kind std or mean\n",
        "  \n",
        "    df_total_to_pivot=df.reset_index()\n",
        "    pivot_by_dates=df_total_to_pivot.pivot(\n",
        "    index='date',\n",
        "    columns='ID',\n",
        "    values='total'\n",
        "    )\n",
        "    describe_by_dates=pivot_by_dates.describe()\n",
        "    group_2000=describe_by_dates.T[describe_by_dates.T[kind]>2000]\n",
        "    group_1000_2000=describe_by_dates.T[(describe_by_dates.T[kind]>1000)&(describe_by_dates.T[kind]<2000)]\n",
        "    group_0=describe_by_dates.T[describe_by_dates.T[kind]==0]\n",
        "    rest_df=describe_by_dates.T[(describe_by_dates.T[kind]<=1000)&(describe_by_dates.T[kind]>0)]\n",
        "    group_0[f'group_by_{kind}_in_total']=1\n",
        "    rest_df[f'group_by_{kind}_in_total']=pd.qcut(rest_df[kind],q=50,labels=range(2,52))\n",
        "    group_1000_2000['fgroup_by_{kind}_in_total']=52\n",
        "    group_2000[f'group_by_{kind}_in_total']=53\n",
        "    df_with_group_by_mean_in_total=pd.concat([group_0,rest_df,group_1000_2000,group_2000])[f'group_by_{kind}_in_total']\n",
        "    return df_with_group_by_mean_in_total.reset_index()\n",
        "\n",
        "df_with_group_by_mean_in_total=create_cluster_by_mean(df,'mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8kQdm6MiYQm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEx5yPTfRvwc"
      },
      "source": [
        "###Extracting features from tsfresh and group using pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWZHQc4nRzh6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def extract_features_per_id_using_tsfresh(df,target='total'):\n",
        "    #'This step we need use in colab because we have a problems with gpu'\n",
        "    logging.info('creating features per id using tsfresh')\n",
        "    path_features_tsfresh=r'D:\\programacion\\Repositorios\\datathon-cajamar-2022\\data\\02_intermediate\\features_tsfresh_per_id.csv'\n",
        "    if os.path.isfile(path_features_tsfresh):\n",
        "        X=pd.read_csv(path_features_tsfresh)\n",
        "    else:\n",
        "        from tsfresh import extract_features, extract_relevant_features, select_features\n",
        "        from tsfresh.utilities.dataframe_functions import impute\n",
        "        from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "        df=df[['ID',target,'date']]\n",
        "        # We are very explicit here and specify the `default_fc_parameters`. If you remove this argument,\n",
        "        # the ComprehensiveFCParameters (= all feature calculators) will also be used as default.\n",
        "        # Have a look into the documentation (https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html)\n",
        "        # or one of the other notebooks to learn more about this.\n",
        "        extraction_settings = ComprehensiveFCParameters()\n",
        "\n",
        "        X = extract_features(df, column_id='ID', column_sort='date',\n",
        "                            default_fc_parameters=extraction_settings,\n",
        "                            # we impute = remove all NaN features automatically\n",
        "                            impute_function=impute)\n",
        "        X.reset_index(inplace=True)\n",
        "        X.rename({'index':'ID'},inplace=True,axis=1)\n",
        "        X.to_csv(path_features_tsfresh,index=False)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vowxoUFtvedv"
      },
      "outputs": [],
      "source": [
        "df_tsfresh=extract_features_per_id_using_tsfresh(df.reset_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83zSHuveVQth"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "\n",
        "def check_pca_in_df(df,extra_name:str='')->None:\n",
        "    \"\"\"remve id,date and total from the df before\n",
        "\n",
        "    Args:\n",
        "        df (_type_): without columns id, date or total\n",
        "        extra_name (str, optional): _description_. Defaults to ''.\n",
        "    \"\"\"    \n",
        "    def save_plot_explained_variance(name_path,pca):\n",
        "\n",
        "        fig=plt.figure(figsize=(15,7))\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "        plt.xlabel('number of components')\n",
        "        plt.ylabel('cumulative explained variance')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    X=df.copy()#.drop(['ID','date','total'],axis=1)\n",
        "    root_path=r'D:\\programacion\\Repositorios\\datathon-cajamar-2022\\data\\08_reporting'\n",
        "    # X=df_features_no_temporal.copy()\n",
        "    # X.drop('total',inplace=True,axis=1)\n",
        "    X.fillna(0,inplace=True)\n",
        "    X.replace(np.inf,0,inplace=True)\n",
        "\n",
        "    X.replace(-np.inf,0,inplace=True)\n",
        "    n_components = 15\n",
        "    ipca = IncrementalPCA(n_components=n_components, batch_size=256)\n",
        "    X_ipca = ipca.fit_transform(X,)\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    \n",
        "    path_ipca=os.path.join(root_path,extra_name+'_ipca.png')\n",
        "    save_plot_explained_variance(path_ipca,ipca)\n",
        "    path_ipca=os.path.join(root_path,extra_name+'_pca.png')\n",
        "    save_plot_explained_variance(path_ipca,ipca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8GnMZpfVX0H"
      },
      "outputs": [],
      "source": [
        "check_pca_in_df(df_tsfresh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB106NVOBNBf"
      },
      "source": [
        "##Plotting a fews plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9PQhKhekijR"
      },
      "outputs": [],
      "source": [
        "df_total_with_bins=pd.merge(df_with_group_by_mean_in_total,df,how='right',left_index=True,right_on='ID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a8pESTTHjbP"
      },
      "outputs": [],
      "source": [
        "#@title plot id per year\n",
        "id_per_plot = 10 #@param {type:\"number\"}\n",
        "plt.figure(figsize=(20,5))\n",
        "for i, id_unique in tqdm(enumerate(df_total_with_bins.sort_values(['group_by_mean_in_total','date']).ID.unique()),desc=\"creating plots\"):\n",
        "    \n",
        "    df_aux=df_total_with_bins[df_total_with_bins['ID'] == id_unique]\n",
        "    bin=df_aux['group_by_mean_in_total'].unique()[0]\n",
        "    plot_series(df_aux.index,df_aux['total'],id_unique)\n",
        "    \n",
        "    if i %id_per_plot==0:\n",
        "        plt.title(f'This bin {str(bin)}')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,5))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjEqldfAuUts"
      },
      "source": [
        "En las gráficas se ve que el id 353 y 2711 hay que corregirlos, siendo en el 353 poniendo los picos a 0 y en el 2711 probablemente poniendolo en positivo el valor negativo y recalcular lo de los bins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi_7RPF_vF0t"
      },
      "source": [
        "##Análisis de dependencia por correlación entre los distintos ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOfa-FTSvdKs"
      },
      "outputs": [],
      "source": [
        "pivot_by_dates.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caGoQ659wLiH"
      },
      "outputs": [],
      "source": [
        "\n",
        "corr_matrix = pivot_by_dates.corr()\n",
        "\n",
        "corr = ((corr_matrix + corr_matrix.T)/2 ).values                        # made symmetric\n",
        "np.fill_diagonal(corr, 1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7iVvtEzZQAN"
      },
      "outputs": [],
      "source": [
        "corr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1P8kfL0vTFF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data_to_correlation=pivot_by_dates.copy()\n",
        "# data_to_correlation.drop(['ID'],axis=1,inplace=True)\n",
        "corr_matrix = data_to_correlation.corr()\n",
        " # Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "s = upper.unstack()\n",
        "so = s.sort_values(kind=\"quicksort\")\n",
        "to_use_in_next_study = so[so>0.8]\n",
        "to_use_in_next_study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TwcgNjK1w2R"
      },
      "source": [
        "Esto nos da información sobre variables que son similares a otras variables, por lo que deberíamos de utilizar esta información en el modelo en algún momento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Fc-OI5vTdP"
      },
      "source": [
        "##trends and stacionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMX-Dsu2yJ0"
      },
      "source": [
        "###using statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHEO2goN1w3S"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "from matplotlib import rcParams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hf9iyoAzqR6"
      },
      "outputs": [],
      "source": [
        "decomposition = sm.tsa.seasonal_decompose(pivot_by_dates[117].dropna())\n",
        "# Store back the results\n",
        "data = decomposition.seasonal\n",
        "plot_series(data.index,data.values,117)\n",
        "# if i%id_per_plots==0 and i>=id_per_plots:\n",
        "\n",
        "plt.title(f'id between {str(i)}')\n",
        "plt.show()\n",
        "plt.figure(figsize=(20,15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPAiEkqZ0bvX"
      },
      "outputs": [],
      "source": [
        "id_per_plots=1\n",
        "plt.figure(figsize=(20,15))\n",
        "for i,ts in enumerate(pivot_by_dates.columns):\n",
        "    decomposition = sm.tsa.seasonal_decompose(pivot_by_dates[ts].dropna())\n",
        "    # Store back the results\n",
        "    data = decomposition.seasonal\n",
        "    plot_series(data.index,data.values,ts)\n",
        "    if i%id_per_plots==0 and i>=id_per_plots:\n",
        "\n",
        "        plt.title(f'id between {str(i)}')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,15))\n",
        "    if i >=100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQaq6b0ozCQt"
      },
      "outputs": [],
      "source": [
        "id_per_plots=10\n",
        "plt.figure(figsize=(20,15))\n",
        "for i,ts in enumerate(pivot_by_dates.columns):\n",
        "    decomposition = sm.tsa.seasonal_decompose(pivot_by_dates[ts].dropna())\n",
        "    # Store back the results\n",
        "    data = decomposition.trend\n",
        "    plot_series(data.index,data.values,ts)\n",
        "    if i%id_per_plots==0 and i>8:\n",
        "\n",
        "        plt.title(f'id between {str(i)}')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,15))\n",
        "    if i >=100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSTt6D7W0pni"
      },
      "outputs": [],
      "source": [
        "id_per_plots=10\n",
        "plt.figure(figsize=(20,15))\n",
        "for i,ts in enumerate(pivot_by_dates.columns):\n",
        "    decomposition = sm.tsa.seasonal_decompose(pivot_by_dates[ts].dropna())\n",
        "    # Store back the results\n",
        "    data = decomposition.resid\n",
        "    plot_series(data.index,data.values,ts)\n",
        "    if i%id_per_plots==0 and i>8:\n",
        "\n",
        "        plt.title(f'id between {str(i)}')\n",
        "        plt.show()\n",
        "        plt.figure(figsize=(20,15))\n",
        "    if i >=100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeYNHFnsNYNU"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics import tsaplots\n",
        "id_per_plots=10\n",
        "plt.figure(figsize=(20,15))\n",
        "for i,ts in enumerate(pivot_by_dates.columns):\n",
        "    tsaplots.plot_acf(pivot_by_dates[ts], lags=90)\n",
        "    \n",
        "    plt.xlabel(\"Lag at k\")\n",
        "    plt.ylabel(\"Correlation coefficient\")\n",
        "    plt.title(f'id between {str(i)}')\n",
        "    plt.show()\n",
        "    plt.figure(figsize=(20,5))\n",
        "    if i >=100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJchoP86Agwj"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import STL\n",
        "id_per_plots=10\n",
        "for i,ts in enumerate(pivot_by_dates.columns):\n",
        "    res=STL(pivot_by_dates[ts],).fit()\n",
        "    fig=res.plot()\n",
        "    fig.set_figheight(10)\n",
        "    fig.set_figwidth(20)\n",
        "    plt.xlabel(\"Lag at k\")\n",
        "    plt.ylabel(\"Correlation coefficient\")\n",
        "    plt.title(f'id between {str(i)}')\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "    if i >=100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aO69EH-A0NR"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qINp3GPtht3G"
      },
      "source": [
        "###using prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s7uqjbjRfxY"
      },
      "outputs": [],
      "source": [
        "pivot_by_dates.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NanSQzewhzM9"
      },
      "outputs": [],
      "source": [
        "df_prophet=pd.DataFrame()\n",
        "df_aux=pivot_by_dates[117]\n",
        "df_prophet[['ds','y']]=df_aux.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4MKqaxQRJJi"
      },
      "outputs": [],
      "source": [
        "df_prophet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftioTRfd8EDW"
      },
      "source": [
        "Solo mostramos los 20 primeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHJxdjnvbMCM"
      },
      "outputs": [],
      "source": [
        "def split_data(data, split_date,column_date=None):\n",
        "    if column_date:\n",
        "        return data[data[column_date] <= split_date].copy(), \\\n",
        "           data[data[column_date] >  split_date].copy()\n",
        "           \n",
        "    else: return data[data.index <= split_date].copy(), \\\n",
        "           data[data.index >  split_date].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMOxDroNeLKB"
      },
      "outputs": [],
      "source": [
        "pivot_by_dates[3].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Iu_etMBisyC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def rmse(y_true,y_pred):\n",
        "    error=mean_squared_error(y_true,y_pred,squared=False)\n",
        "    return error\n",
        "error_df=pd.DataFrame()\n",
        "for i,ts in tqdm(enumerate(pivot_by_dates.columns)): \n",
        "    \n",
        "    df_prophet=pd.DataFrame()\n",
        "    df_aux=pivot_by_dates[ts]\n",
        "    train, test = split_data(df_aux, '2020-01-24') \n",
        "    df_prophet[['ds','y']]=train.reset_index()\n",
        "    m = Prophet(yearly_seasonality=False,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False\n",
        "        )\n",
        "    m.fit(df_prophet)\n",
        "    future=m.make_future_dataframe(7)\n",
        "    forecast = m.predict(future)\n",
        "    forecast['yhat']=np.where(forecast['yhat']<0,\n",
        "                              0,\n",
        "                              forecast['yhat'])\n",
        "    train_forecast, test_forecast = split_data(forecast, '2020-01-24','ds') \n",
        "    error=rmse(test,test_forecast.yhat)\n",
        "    error_df.loc[ts,'Prophet_without_exogenous']=error\n",
        "    print(error_df.head())\n",
        "    plot_series(train.index,train,f'{ts} train_true')\n",
        "    plot_series(test.index,test,f'{ts} test_true')\n",
        "    plot_series(train_forecast.ds,train_forecast['yhat'],f'{ts} train_predict')\n",
        "    plot_series(test_forecast.ds,test_forecast['yhat'],f'{ts} test_predict')\n",
        "    plt.show()\n",
        "    # fig1 = m.plot(forecast)\n",
        "    # fig1.suptitle('id '+str(ts))\n",
        "    # fig1.show()\n",
        "    # print(\"\\n\"*1)\n",
        "    # fig2 = m.plot_components(forecast)\n",
        "    # fig2.suptitle('id '+str(ts))\n",
        "    # print(\"\\n\"*3)\n",
        "    if i>20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KVFEfMyctMY"
      },
      "outputs": [],
      "source": [
        "error_df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuLMdaovlxTv"
      },
      "outputs": [],
      "source": [
        "error_df.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OouX9RRw4ZQ-"
      },
      "source": [
        "### with pycaret --> time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFR7S-y64k3T"
      },
      "outputs": [],
      "source": [
        "from pycaret import time_series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_eD6HDpOPQE"
      },
      "outputs": [],
      "source": [
        "fh=7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pcdhyKw-Yyh"
      },
      "outputs": [],
      "source": [
        "for i,ts in tqdm(enumerate(pivot_by_dates.columns),mininterval=5,desc='creating_models'): \n",
        "    # df_to_time_regression_original=df[df['ID']==id_unique].copy()\n",
        "    df_to_time_regression_original=pivot_by_dates[ts]\n",
        "    train, test = split_data(df_to_time_regression_original, '2020-01-24') \n",
        "    \n",
        "    time_series.setup(\n",
        "    data=train, fh=fh,\n",
        "    enforce_exogenous=False,\n",
        "    # Set defaults for the plots ----\n",
        "    # fig_kwargs={\"renderer\": \"notebook\", \"width\": 1000, \"height\": 600},\n",
        "    session_id=42,\n",
        "    \n",
        "    )\n",
        "    best=time_series.compare_models(turbo=False,sort='RMSE',round=2,\n",
        "                                    exclude=[\n",
        "                                             'huber_cds_dt',\n",
        "                                             'tbats',\n",
        "                                             'bats',\n",
        "                                             'auto_arima'])\n",
        "\n",
        "    print('This is the id:', id_unique)\n",
        "    path=r'D:\\programacion\\Repositorios\\datathon-cajamar-2022\\data\\06_models\\best_model_'+str(id_unique)\n",
        "\n",
        "    \n",
        "    best=time_series.finalize_model(best)\n",
        "    time_series.save_model(best,path)\n",
        "\n",
        "    predictions=time_series.predict_model(best,fh=7,return_pred_int=False,round=2)\n",
        "    \n",
        "    error=rmse(test,test_forecast.yhat)\n",
        "    error_df.loc[ts,'pycaret_time_series_without_exogenous']=error\n",
        "    plot_series(train.index,train,f'{ts} train_true')\n",
        "    plt.show()\n",
        "    plot_series(test.index,test,f'{ts} test_true')\n",
        "    # plot_series(train_forecast.ds,train_forecast['yhat'],f'{ts} train_predict')\n",
        "    plot_series(predictions.index,predictions,f'{ts} test_predict')\n",
        "\n",
        "    if id_unique>20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EqGwvIdn6Is"
      },
      "outputs": [],
      "source": [
        "error=rmse(test,test_forecast.yhat)\n",
        "error_df.loc[ts,'pycaret_time_series']=error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JFQV9fLC1DU"
      },
      "outputs": [],
      "source": [
        "error_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzU5WYfAbu_J"
      },
      "source": [
        "#Regression\n",
        "###solo usar si creamos distintas caracteriticas con prophet por ejemplo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULHGrtVAr3bS"
      },
      "source": [
        "#as regression problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq15i3MannNG"
      },
      "source": [
        "##features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE1kZR3zPAqR"
      },
      "outputs": [],
      "source": [
        "target= 'total'\n",
        "key_metric='RMSE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4e2uY3AgqH8"
      },
      "outputs": [],
      "source": [
        "#@title extract features\n",
        "\n",
        "from src.create_the_output_file import create_outputfile\n",
        "from src.generate_df import generate_df\n",
        "from src.pipeline import  train_systems\n",
        "from src.predict import predict\n",
        "from src.utils import extract_n_relevant_features\n",
        "from src.create_features import create_features,extract_features_per_id_using_tsfresh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Variables\n"
      ],
      "metadata": {
        "id": "JG5V_69U64DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgvBFe5LsvhC"
      },
      "outputs": [],
      "source": [
        "df=generate_df('/content/Modelar_UH2022.txt','W')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "N_7iE8fB7oqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hep33_5vadn"
      },
      "outputs": [],
      "source": [
        "aggregated=create_features(df,'W')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA-mjtvTvrJ_"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "data_without_total=aggregated.copy()\n",
        "data_without_total.drop(target,axis=1,inplace=True)\n",
        "corr_matrix = data_without_total.corr()\n",
        " # Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
        "print(len(to_drop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-X8GHADSR3S"
      },
      "outputs": [],
      "source": [
        "for value_max in range(60,100,5):\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] >value_max/100)]\n",
        "    print('value_max',value_max,'columns to drop',len(to_drop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn3SmY1f6e0Q"
      },
      "outputs": [],
      "source": [
        "#@title to get df_tsfresh\n",
        "is_needed_df_tsfresh = True #@param {type:\"boolean\"}\n",
        "if is_needed_df_tsfresh:\n",
        "    df_tsfresh=extract_features_per_id_using_tsfresh(df)\n",
        "    df_tsfresh.reset_index(inplace=True)\n",
        "    df_tsfresh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZPl8gDzvzc3"
      },
      "outputs": [],
      "source": [
        "#@title Outliers\n",
        "from sklearn.ensemble import IsolationForest\n",
        "outliers_fraction=0.1\n",
        "method_forest=IsolationForest(contamination=outliers_fraction)\n",
        "df_to_use=aggregated[['total','ID','date']]\n",
        "df_aux=pd.DataFrame()\n",
        "for id_unique in df_to_use.ID.unique():\n",
        "    df_per_id=df_to_use[df_to_use['ID']==id_unique].copy()\n",
        "    add_later=df_per_id['date']\n",
        "    df_per_id.drop('date',axis=1,inplace=True)\n",
        "    data_to_correction=aggregated[aggregated['ID']==id_unique]['14d_roll_mean']\n",
        "    yhat = method_forest.fit_predict(df_per_id)\n",
        "    mask = yhat != -1\n",
        "    df_per_id['total']=np.where(mask,\n",
        "                                df_per_id.total,\n",
        "                                data_to_correction\n",
        "                            )\n",
        "    df_per_id.loc[:,'date']=add_later\n",
        "    df_aux=pd.concat([df_aux,df_per_id],axis=0)\n",
        "    \n",
        "df_aux.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q19MQcjr1alS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Styy9ULRzKEp"
      },
      "outputs": [],
      "source": [
        "aggregated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYlzQdOWvCoy"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "data_to_correlation=df_features_no_temporal.copy()\n",
        "data_to_correlation.drop(['ID'],axis=1,inplace=True)\n",
        "corr_matrix = data_to_correlation.corr()\n",
        " # Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "to_use_in_next_study = [column for column in upper.columns if any(upper[column] < 0.1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KaVAutYjsdW"
      },
      "outputs": [],
      "source": [
        "# Drop Marked Features\n",
        "to_drop=[]\n",
        "# to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
        "print(len(to_drop))\n",
        "print(aggregated.shape)\n",
        "aggregated_after_drop = aggregated.drop(to_drop, axis=1)\n",
        "print(aggregated_after_drop.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WL-PiHytfPI"
      },
      "outputs": [],
      "source": [
        "train, test = split_data(aggregated_after_drop, '2020-01-15') \n",
        "train_aux, test_aux = split_data(df_aux, '2020-01-15') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=aggregated_after_drop"
      ],
      "metadata": {
        "id": "5xBClHAkIle_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMqYvtqh3mP_"
      },
      "outputs": [],
      "source": [
        "train['total']=train_aux['total']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-Jf_qJoGUtW"
      },
      "outputs": [],
      "source": [
        "for id in aggregated.ID.unique():\n",
        "    train_per_id=train[train['ID']==id]\n",
        "    test_per_id=test[test['ID']==id]\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.xlabel('date')\n",
        "    plt.ylabel('Water_used')\n",
        "    plt.plot(train_per_id['date'],train_per_id[target],label='train')\n",
        "    plt.plot(test_per_id['date'],test_per_id[target],label='test')\n",
        "    plt.title(id)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if id>10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEH7lfKZOf0H"
      },
      "source": [
        "##train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6VnVq_xQdqR"
      },
      "outputs": [],
      "source": [
        "from pycaret.regression import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K59WvXluRCeg"
      },
      "outputs": [],
      "source": [
        "#@title Pycaret setup\n",
        "reg = setup(data = train, \n",
        "             target = target,\n",
        "             numeric_imputation = 'mean',\n",
        "            ignore_features=['ID','date',\n",
        "                            #  'dayofmonth',\n",
        "                            #  'month'\n",
        "                            #  'dayofweek'\n",
        "                             ],\n",
        "            #faltará añadir más datos en numericos de estacionalidd, tendencia, etc por cada ID mediante prophet\n",
        "             categorical_features = [\n",
        "                                     'dayofweek',\n",
        "                                    #  'quarter','month',\n",
        "                                    #  'year',\n",
        "                                    #  'dayofyear',\n",
        "                                     'dayofmonth',\n",
        "                                    #  'weekofyear',\n",
        "                                    #  'ID'\n",
        "                                     ]  , \n",
        "            normalize=True,\n",
        "            transformation = True, transform_target = False, \n",
        "            transform_target_method='yeo-johnson',\n",
        "                  combine_rare_levels = True, rare_level_threshold = 0.1,\n",
        "                #   remove_multicollinearity = True, multicollinearity_threshold = 0.95, \n",
        "            # feature_selection=True,\n",
        "            create_clusters=True,\n",
        "            cluster_iter=5,\n",
        "            # pca=True,\n",
        "             silent = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RhcdX_ax2PMH"
      },
      "outputs": [],
      "source": [
        "#@title Models available\n",
        "see_models_available = False #@param {type:\"boolean\"}\n",
        "if see_models_available:\n",
        "    all_models = models()\n",
        "    print(all_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MyapF0RROAW"
      },
      "outputs": [],
      "source": [
        "# returns best models - takes a little time to run\n",
        "top = compare_models(sort=key_metric,exclude=[\n",
        "                                              'rf',\n",
        "                                              'huber',\n",
        "                                            #   'br',\n",
        "                                              'ada',\n",
        "                                              'et',\n",
        "                                              'xgboost',\n",
        "                                              'catboost',\n",
        "                                            #   'knn',\n",
        "                                            #   'gbr'\n",
        "                                     \n",
        "                                                # 'ridge',\n",
        "                                                # 'lasso',\n",
        "                                                # 'omp',\n",
        "                                                # 'par',\n",
        "                                                # 'en',\n",
        "                                                # 'lar'\n",
        "                                              ],\n",
        "                     round=2,\n",
        "                     turbo=True\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wvXlwB1RVUk"
      },
      "outputs": [],
      "source": [
        "#we create a model using light gbm\n",
        "lightgbm = create_model('lightgbm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXLP1WEjRZGb"
      },
      "outputs": [],
      "source": [
        "tuned_lightgbm = tune_model(lightgbm,early_stopping=True,optimize='rmse') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MBaVaXGkEmL"
      },
      "outputs": [],
      "source": [
        "evaluate_model(lightgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLX3hJmJk8bO"
      },
      "outputs": [],
      "source": [
        "evaluate_model(tuned_lightgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTZNZ_VCRa8-"
      },
      "outputs": [],
      "source": [
        "plot_model(lightgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPRm_xQKRcip"
      },
      "outputs": [],
      "source": [
        "plot_model(lightgbm, plot = 'error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo5KW2CvnAEz"
      },
      "outputs": [],
      "source": [
        "def extract_n_relevant_features(model,n=20):\n",
        "    X_train=get_config('X_train',)\n",
        "    feats = {} # a dict to hold feature_name: feature_importance\n",
        "    for feature, importance in zip(X_train.columns, model.feature_importances_):\n",
        "        feats[feature] = importance #add the name/value pair \n",
        "\n",
        "    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
        "    return importances.sort_values(by='Gini-importance',ascending=False).head(n)#.plot(kind='bar', rot=45)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXNQvZ6md1Yu"
      },
      "outputs": [],
      "source": [
        "plot_model(lightgbm, plot='feature_all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBnTfgpapR2E"
      },
      "outputs": [],
      "source": [
        "n=30\n",
        "features=extract_n_relevant_features(lightgbm,n)\n",
        "print(features.head(n))\n",
        "features.index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjmK-G7zReRA"
      },
      "outputs": [],
      "source": [
        "plot_model(tuned_lightgbm, plot='feature_all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rhzFGoYpTdQ"
      },
      "outputs": [],
      "source": [
        "n=35\n",
        "features=extract_n_relevant_features(tuned_lightgbm,n)\n",
        "print(features.head(n))\n",
        "features.index.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg4KBCpklok_"
      },
      "outputs": [],
      "source": [
        "print(dir(tuned_lightgbm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJu3xgoz_m5B"
      },
      "outputs": [],
      "source": [
        "plot_model(tuned_lightgbm, plot='rfe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPOZPy7lRfs-"
      },
      "outputs": [],
      "source": [
        "predict_model(tuned_lightgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPWQlSqTUDfM"
      },
      "outputs": [],
      "source": [
        "final_lightgbm = finalize_model(tuned_lightgbm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoEO973OULk1"
      },
      "outputs": [],
      "source": [
        "unseen_predictions = predict_model(final_lightgbm, data=test)\n",
        "unseen_predictions.head()\n",
        "unseen_predictions.loc[unseen_predictions['Label'] < 0, 'Label'] = 0 #removing any negative values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLAa8c7nUNSv"
      },
      "outputs": [],
      "source": [
        "def plot_series(time, series,i, format=\"-\", start=0, end=None):\n",
        "\n",
        "    \n",
        "    \n",
        "    plt.plot(time[start:end], series[start:end], format,label=i)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Sales (Water)\")\n",
        "    plt.legend()\n",
        "def plot_results(test,unseen_predictions,stop:int=5,label:str='Baseline'):\n",
        "    for id in test.ID.unique():\n",
        "        plt.figure(figsize=(15,7))\n",
        "        test_per_id=test[test['ID']==id]\n",
        "        unseen_predictions_per_id=unseen_predictions[unseen_predictions['ID']==id]\n",
        "        plot_series(test_per_id['date'], test_per_id[target],f\"True {id}\")\n",
        "        #plot_series(train['ds'],train['y'])\n",
        "        plot_series(test_per_id['date'], unseen_predictions_per_id['Label'],f\"{label} {id}\")\n",
        "        plt.show()\n",
        "        print('\\n')\n",
        "        if id>=stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7c6cww_fWS3"
      },
      "outputs": [],
      "source": [
        "plot_results(test,unseen_predictions,15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfYN7rrfbQ_4"
      },
      "source": [
        "###Blending Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCdNkGjDX63H"
      },
      "outputs": [],
      "source": [
        "gbr = create_model('gbr', verbose = False)\n",
        "# rf = create_model('rf', verbose = False)\n",
        "lightgbm = create_model('lightgbm', verbose = False)\n",
        "xgb = create_model('xgboost',verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhSHVTgOXQC5"
      },
      "outputs": [],
      "source": [
        "# tuned_rf = tune_model(rf)\n",
        "tuned_gbr = tune_model(gbr)\n",
        "tuned_lightgbm = tune_model(lightgbm)\n",
        "tuned_xgb = tune_model(xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxzAN1aZZ9BT"
      },
      "outputs": [],
      "source": [
        "blend_specific = blend_models(estimator_list = [tuned_lightgbm,tuned_gbr,tuned_xgb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tsVwSwbKHMn"
      },
      "outputs": [],
      "source": [
        "blend_specific"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K5LgwcMaDYC"
      },
      "outputs": [],
      "source": [
        "predict_model(blend_specific);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsXbVr7AaD3T"
      },
      "outputs": [],
      "source": [
        "final_model = finalize_model(blend_specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkg6RN_TaGnq"
      },
      "outputs": [],
      "source": [
        "unseen_predictions_2 = predict_model(final_model, data=test, round=0)\n",
        "unseen_predictions_2.loc[unseen_predictions_2['Label'] < 0, 'Label'] = 0\n",
        "unseen_predictions_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_TABiuPafF-"
      },
      "outputs": [],
      "source": [
        "plot_results(test,unseen_predictions_2,label='blend')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUDGDWzbLXh"
      },
      "source": [
        "###Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bC0GmQUbME-"
      },
      "outputs": [],
      "source": [
        "stack_1 = stack_models([tuned_lightgbm,tuned_gbr,tuned_xgb])\n",
        "predict_model(stack_1);\n",
        "final_stack_1 = finalize_model(stack_1)\n",
        "unseen_predictions_3 = predict_model(final_stack_1, data=test, round=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8LkqkvOdfic"
      },
      "outputs": [],
      "source": [
        "unseen_predictions_3.loc[unseen_predictions_3['Label'] < 0, 'Label'] = 0\n",
        "unseen_predictions_3.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9Qwd0Mdeg6V"
      },
      "outputs": [],
      "source": [
        "plot_results(test,unseen_predictions_3,label='stacking')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHzz9DZyejQm"
      },
      "outputs": [],
      "source": [
        "def plot_all_results(test,unseen_predictions_dict:dict,stop:int=5,):\n",
        "    for id in test.ID.unique():\n",
        "        plt.figure(figsize=(15,7))\n",
        "        test_per_id=test[test['ID']==id]\n",
        "        # unseen_predictions_per_id=unseen_predictions[unseen_predictions['ID']==id]\n",
        "        plot_series(test_per_id['date'], test_per_id[target],f\"True {id}\")\n",
        "        #plot_series(train['ds'],train['y'])\n",
        "        for label,df_unseen in unseen_predictions_dict.items():\n",
        "            df_unseen=df_unseen[df_unseen['ID']==id]\n",
        "            plot_series(test_per_id['date'], df_unseen['Label'],f\"{label} {id}\")\n",
        "        plt.show()\n",
        "        print('\\n')\n",
        "        if id>=stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U9G2OXlejSn"
      },
      "outputs": [],
      "source": [
        "unseen_predictions_dict={\n",
        "    'baseline':unseen_predictions,\n",
        "    'blend': unseen_predictions_2, \n",
        "    'stacking': unseen_predictions_3, \n",
        "}\n",
        "plot_all_results(test,unseen_predictions_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbxyGnDOrlmf"
      },
      "source": [
        "Train only timeseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDmnkbrBrsY9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVSx6dmNKVVn"
      },
      "source": [
        "##Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs2GIObMpQXi"
      },
      "source": [
        "necesito las cuatro formulas que uso para extraer características\n",
        "\n",
        "basic_temporal_features -> hecho\n",
        "extract_features_per_id_using_tsfresh -> no depende del tiempo por lo que coger el df\n",
        "extract_features_from_prophet\n",
        "extra_features_manually_no_temporal_variables -> no depende del tiempo por lo que coger el df\n",
        "extra_features_manually_temporal_variables -> hecho\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI8NwJTPKbSO"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDh62OiGK5ls"
      },
      "outputs": [],
      "source": [
        "start='2020-02-01'\n",
        "last_date='2020-02-14'\n",
        "freq='D'\n",
        "periods=14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHJYd0INK4cK"
      },
      "outputs": [],
      "source": [
        "dates = pd.date_range(\n",
        "            start=start,\n",
        "            periods=periods,  # An extra in case we include start\n",
        "            freq=freq)\n",
        "unique_ids=df.ID.unique()\n",
        "# dates = dates[dates > last_date]  # Drop start if equals last_date\n",
        "# dates = dates[:periods]  # Return correct number of periods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0g7iSCZLyPR"
      },
      "outputs": [],
      "source": [
        "df_aux=pd.DataFrame()\n",
        "for id in unique_ids:\n",
        "    df_per_id=pd.DataFrame()\n",
        "    df_per_id.index=dates\n",
        "    df_per_id.index = df_per_id.index.set_names(['date'])\n",
        "    df_per_id.reset_index(inplace=True)\n",
        "    df_per_id['ID']=id\n",
        "    df_per_id['year']=df_per_id['date'].dt.year\n",
        "    df_per_id['month']=df_per_id['date'].dt.month\n",
        "    df_per_id['day']=df_per_id['date'].dt.day\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd9E_5l1NAX5"
      },
      "outputs": [],
      "source": [
        "df_to_test=pd.concat([df,df_per_id,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wCS-Vn-R6Kn"
      },
      "outputs": [],
      "source": [
        "df_to_test=df_to_test[df_to_test['date']<'2020-02-02']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rr-9IDSSE0S"
      },
      "outputs": [],
      "source": [
        "df_to_test[df_to_test['date']=='2020-02-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc3z103-m4Rr"
      },
      "outputs": [],
      "source": [
        "df_to_test=basic_temporal_features(df_to_test)\n",
        "features_using_prophet=extract_features_from_prophet(df_to_test,is_to_predict=True) #habría que ir cambiando el total y volviendo a iterar esto por cada día\n",
        "# features_manually_no_temporal=extra_features_manually_no_temporal_variables(df_to_test)\n",
        "features_manually_temporal_variables=extra_features_manually_temporal_variables(df_to_test)\n",
        "#añadir la suma del mes anterior\n",
        "#añadir la suma de la semana anterior\n",
        "#añadir la suma del trimestre anterior\n",
        "\n",
        "#add data from tsfresh y from prophet (en ambos casos se hará por id y luego se mergeará a ese ID)\n",
        "X = df_to_test[['dayofweek','quarter','month',\n",
        "        # 'year',  #only we have a year, and that is not enought to the model learn what hapen to the next year\n",
        "        'dayofyear','dayofmonth','weekofyear','total','ID','date']]\n",
        "\n",
        "# X=pd.merge(X,features_tsfresh_per_id,right_index=True,left_on='ID')\n",
        "# X=pd.merge(X,features_using_prophet,left_on=['ID','date'],right_on=['ID','ds'])\n",
        "# X=pd.merge(X,features_manually_no_temporal,right_on='ID',left_on='ID')\n",
        "df2=pd.merge(X,features_manually_temporal_variables,right_on=['ID','date'],left_on=['ID','date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4F5ZOXXqj5p"
      },
      "outputs": [],
      "source": [
        "df2[df2['date']=='2020-02-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wfISVKDqcuQ"
      },
      "outputs": [],
      "source": [
        "df2.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T5j1cgeVuGS"
      },
      "outputs": [],
      "source": [
        "features_using_prophet=extract_features_from_prophet(df_to_test,is_to_predict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kpSODbJtGuP"
      },
      "outputs": [],
      "source": [
        "features_using_prophet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIoWEwTESLFZ"
      },
      "outputs": [],
      "source": [
        "df_to_test=extra_features_manually_temporal_variables(df_to_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CucXuaB_m8N3"
      },
      "outputs": [],
      "source": [
        "b=df_to_test[df_to_test['ID']==4]\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G2rqKewS5vK"
      },
      "outputs": [],
      "source": [
        "df_per_id=df_to_test[df_to_test['ID']==4]\n",
        "# df_per_id.fillna(0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZsa13oLS4ZT"
      },
      "outputs": [],
      "source": [
        "df_per_id[\"1d_roll_sum\"] = df_per_id[target].rolling(window=1,closed='left').sum().fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-DOBidmtxu"
      },
      "outputs": [],
      "source": [
        "df_per_id['shifted_1']=df_per_id[target].shift(periods=1).fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEmADfgKTB1E"
      },
      "outputs": [],
      "source": [
        "df_per_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X5TuEapSRa6"
      },
      "outputs": [],
      "source": [
        "a[a['date']=='2020-02-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tvIqcMVfZ_N"
      },
      "outputs": [],
      "source": [
        "predict_model(final_model, data=test, round=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCf_ZeBPbtK4"
      },
      "source": [
        "#Time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0vCjeiJZn3P"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pycaret.internal.pycaret_experiment import TimeSeriesExperiment\n",
        "\n",
        "from pycaret.time_series import TSForecastingExperiment\n",
        "# exp=from pycaret.internal.pycaret_experiment import TimeSeriesExperiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbvfBlG4RcVU"
      },
      "outputs": [],
      "source": [
        "from pycaret import time_series\n",
        "import numpy as np\n",
        "from pipeline import split_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3l71tDesRb7"
      },
      "outputs": [],
      "source": [
        "df=get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmKA79E8LlvV"
      },
      "outputs": [],
      "source": [
        "df_correction=cheking_outliers(df.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDGMMZDZZqF7"
      },
      "outputs": [],
      "source": [
        "fh = 7 # or alternately fh = np.arange(1,13)\n",
        "fold = 5\n",
        "target='total'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L0UcCUPtoQu"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuWzikEQ9zM"
      },
      "outputs": [],
      "source": [
        "results=pd.DataFrame()\n",
        "for id_unique in tqdm(df.ID.unique(),mininterval=5,desc='creating_models'):\n",
        "    df_to_time_regression_original=df[df['ID']==id_unique].copy()\n",
        "    df_correction=cheking_outliers(df.copy())\n",
        "    df_correction_per_id=df_correction[df_correction['ID']==id_unique].copy()\n",
        "    df_to_time_regression=df_to_time_regression_original.copy()\n",
        "    df_to_time_regression.index=df_to_time_regression.date\n",
        "    df_correction_per_id.index=df_to_time_regression.date\n",
        "\n",
        "    df_train_correction,df_test_correction=split_data(df_correction_per_id.copy(), '2020-01-15')\n",
        "    \n",
        "    df_train,df_test=split_data(df_to_time_regression.copy(), '2020-01-15') \n",
        "    print(df_train.head())\n",
        "    print(df_train_correction.head())\n",
        "    df_train['total']=df_train_correction['total']\n",
        "    print(df_train.head())\n",
        "    df_train.drop(['date','ID'],inplace=True,axis=1)\n",
        "    df_test.drop(['date','ID'],inplace=True,axis=1)\n",
        "    time_series.setup(\n",
        "    data=df_train, target=target, fh=fh,\n",
        "    enforce_exogenous=False,\n",
        "    # Set defaults for the plots ----\n",
        "    # fig_kwargs={\"renderer\": \"notebook\", \"width\": 1000, \"height\": 600},\n",
        "    session_id=42,\n",
        "    \n",
        "    )\n",
        "    best=time_series.compare_models(turbo=False,sort='RMSE',round=2,\n",
        "                                    exclude=[\n",
        "                                             'huber_cds_dt',\n",
        "                                             'tbats',\n",
        "                                             'bats',\n",
        "                                             'auto_arima'])\n",
        "\n",
        "    print('This is the id:', id_unique)\n",
        "    path=r'D:\\programacion\\Repositorios\\datathon-cajamar-2022\\data\\06_models\\best_model_'+str(id_unique)\n",
        "\n",
        "    time_series.save_model(best,path)\n",
        "    best=time_series.finalize_model(best)\n",
        "    predictions=time_series.predict_model(best,fh=14,return_pred_int=False,round=2)\n",
        "    df_test_to_merge=df_test.head(14)\n",
        "    df_test_to_merge.index=predictions.index\n",
        "    results_temporal=pd.DataFrame()\n",
        "    results_temporal=pd.merge(df_test_to_merge,predictions,left_index=True,right_index=True)\n",
        "    results_temporal['ID']=id_unique\n",
        "\n",
        "    results=pd.concat([results,results_temporal])\n",
        "    if id_unique>3:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0wIfoYKiD9w"
      },
      "outputs": [],
      "source": [
        "result_with_date=results.reset_index()\n",
        "result_with_date.rename({'index':'date'},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwDh_xM7hm6s"
      },
      "outputs": [],
      "source": [
        "for id_unique in result_with_date.ID.unique():\n",
        "    results_id=result_with_date[result_with_date['ID']==id_unique]\n",
        "    plt.figure(figsize=(17,7))\n",
        "    plot_series(results_id.index.to_list(), results_id['total'],f\"True {id_unique}\")\n",
        "            #plot_series(train['ds'],train['y'])\n",
        "    plot_series(results_id.index.to_list(), results_id['y_pred'],f\"prediction {id_unique}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBkL1Esp3cfW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "errors={}\n",
        "errors['ID']=[]\n",
        "errors['RMSE']=[]\n",
        "for id_unique in result_with_date.ID.unique():\n",
        "    error_per_id=pd.DataFrame()\n",
        "    results_id=result_with_date[result_with_date['ID']==id_unique]\n",
        "    \n",
        "    error=mean_squared_error(results_id['y_pred'],results_id['total'],squared=False)\n",
        "    errors['ID'].append(id_unique)\n",
        "    errors['RMSE'].append(error)\n",
        "    \n",
        "    # error_per_id['ID']=id_unique\n",
        "    # error_per_id['RMSE']=error\n",
        "    # errors_per_id=pd.concat([errors_per_id,error_per_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOH6-Ycr7ca4"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmaWCdiKhgpq"
      },
      "outputs": [],
      "source": [
        "errors_per_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgNCpz0rMwAo"
      },
      "outputs": [],
      "source": [
        "time_series.plot_model(plot=\"diagnostics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Blkw7spMxlI"
      },
      "outputs": [],
      "source": [
        "# Plot the target variable along with any exogenous variables ----\n",
        "# Becasue the data is quite huge, plotting an interactive plot can slow down the notebook.\n",
        "# Hence, we will revert to a static renderer for this plot\n",
        "time_series.plot_model(fig_kwargs={\"renderer\": \"png\", \"height\": 1200})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmu27mEZM2WH"
      },
      "outputs": [],
      "source": [
        "# Plots original data with first difference (order d = 1) by default\n",
        "time_series.plot_model(\n",
        "    plot=\"diff\",\n",
        "    data_kwargs={\"acf\": True, \"pacf\": True, \"periodogram\": True}    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB8BBxYBF5RP"
      },
      "outputs": [],
      "source": [
        "time_series.plot_model(\n",
        "    plot=\"diff\",\n",
        "    data_kwargs={\"lags_list\":[1, [1, 24]],\"acf\": True, \"pacf\": True, \"periodogram\": True}    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkJGNwd1H69N"
      },
      "outputs": [],
      "source": [
        "time_series.plot_model(plot=\"ccf\", fig_kwargs={\"height\": 1000})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYixPtS4NRsL"
      },
      "source": [
        "##model univariate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VluBhrFPatqn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJrXTTaNKS4m"
      },
      "outputs": [],
      "source": [
        "time_series.plot_model(best,'forecast',fig_kwargs={\"renderer\": \"png\", \"height\": 1200}  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAmKNMcDbFLp"
      },
      "outputs": [],
      "source": [
        "eda.plot_model(plot=\"decomp_classical\")\n",
        "eda.plot_model(plot=\"decomp_classical\", data_kwargs={'type': 'multiplicative'})\n",
        "eda.plot_model(plot=\"decomp_stl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7L4dFLJbJc2"
      },
      "outputs": [],
      "source": [
        "# Show the train-test splits on the dataset\n",
        "# Internally split - len(fh) as test set, remaining used as test set\n",
        "eda.plot_model(plot=\"train_test_split\",fig_kwargs={\"renderer\": \"png\", \"height\": 1200} )\n",
        "\n",
        "# Show the Cross Validation splits inside the train set\n",
        "eda.plot_model(plot=\"cv\",fig_kwargs={\"renderer\": \"png\", \"height\": 1200})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ODNH0vobLXO"
      },
      "outputs": [],
      "source": [
        "# Plot diagnostics\n",
        "eda.plot_model(plot=\"diagnostics\",fig_kwargs={\"renderer\": \"png\", \"height\": 1200} )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jvsOO0KbNZB"
      },
      "outputs": [],
      "source": [
        "# Plot differences along with diagnostics such as ACF and PACF\n",
        "\n",
        "# Row 1: Original\n",
        "# Row 2: d = 1\n",
        "# Row 3: d = 2\n",
        "eda.plot_model(plot=\"diff\", data_kwargs={\"order_list\": [1, 2], \"pacf\": True},fig_kwargs={\"renderer\": \"png\", \"height\": 1200} )\n",
        "\n",
        "# Row 1: Original\n",
        "# Row 2: d = 1\n",
        "# Row 3: First (d = 1) then (D = 1, s = 12)\n",
        "#   - Corresponds to applying a standard first difference to handle trend, and\n",
        "#     followed by a seasonal difference (at lag 12) to attempt to account for\n",
        "#     seasonal dependence.\n",
        "# Ref: https://www.sktime.org/en/v0.8.0/api_reference/modules/auto_generated/sktime.transformations.series.difference.Differencer.html\n",
        "eda.plot_model(plot=\"diff\", data_kwargs={\"lags_list\": [[1], [1, 12]], \"acf\": True, \"pacf\": True},fig_kwargs={\"renderer\": \"png\", \"height\": 1200} )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRAPVnLqaQ_V"
      },
      "outputs": [],
      "source": [
        "eda.plot_model(best, plot = 'insample',fig_kwargs={\"renderer\": \"png\", \"height\": 1200} )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVEx0WWePAhB"
      },
      "outputs": [],
      "source": [
        "df=get_data()\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7w9MkTEp1ti"
      },
      "outputs": [],
      "source": [
        "len(df.ID.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DuJ1wtf6vas"
      },
      "source": [
        "#Output format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-WdbbOW3RXh"
      },
      "outputs": [],
      "source": [
        "df_original=df.copy()\n",
        "df_original.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjterEsA3uwy"
      },
      "outputs": [],
      "source": [
        "df=get_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ0fg7lb3wBA"
      },
      "outputs": [],
      "source": [
        "basic_temporal_features(df).total.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tEVagqH4Fq3"
      },
      "outputs": [],
      "source": [
        "test[['ID','2d_roll_sum','shifted_diff_1','shifted_diff_2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZFXWVMUDijP"
      },
      "outputs": [],
      "source": [
        "print(df_full_to_get_temporal_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QCtjgN46zAa"
      },
      "outputs": [],
      "source": [
        "#we need predict 14 days of February 7 days to predict and 2 weeks\n",
        "def get_range_to_predic():\n",
        "    start='2020-02-01'\n",
        "    last_date='2020-02-14'\n",
        "    freq='D'\n",
        "    periods=14\n",
        "    dates = pd.date_range(\n",
        "        start=start,\n",
        "        periods=periods,  # An extra in case we include start\n",
        "        freq=freq)\n",
        "\n",
        "    return dates\n",
        "\n",
        "# model=regression.load_model(model)\n",
        "dates=get_range_to_predic()\n",
        "unique_id=df.ID.unique()\n",
        "for day_to_predict in dates:\n",
        "    print(day_to_predict)\n",
        "    df_aux=pd.DataFrame()\n",
        "    for id_unique in unique_id:\n",
        "        df_per_id=pd.DataFrame()\n",
        "        df_per_id.index=dates\n",
        "        df_per_id.index = df_per_id.index.set_names(['date'])\n",
        "        df_per_id.reset_index(inplace=True)\n",
        "        df_per_id['ID']=id_unique\n",
        "\n",
        "        df_per_id=df_per_id[df_per_id['date']==day_to_predict]\n",
        "\n",
        "        df_aux=pd.concat([df_aux,df_per_id])\n",
        "    df_aux.loc[:,'date']=pd.to_datetime(df_aux['date'],errors='raise',format='%Y-%m-%d')\n",
        "\n",
        "    df_to_predict=basic_temporal_features(df_aux)\n",
        "\n",
        "    df_full_to_get_temporal_features=pd.concat([df_to_predict,df])\n",
        "    df_full_to_get_temporal_features.sort_values(['ID','date'],inplace=True)\n",
        "\n",
        "    df_full=create_features(df_full_to_get_temporal_features)\n",
        "    df_to_predict=df_full[df_full['date']==pd.to_datetime(day_to_predict)]\n",
        "    logging.debug(df_to_predict.shape)\n",
        "    results=pd.DataFrame()\n",
        "    \n",
        "\n",
        "        \n",
        "    df_to_predict_per_id=df_to_predict[df_to_predict['ID']==id_unique]\n",
        "    results=predict_model(tuned_lasso,df_to_predict)\n",
        "    \n",
        "\n",
        "    print(results.shape)\n",
        "    print(results[['ID','total','date','Label']].tail(15))\n",
        "    print(results[results['date']==pd.to_datetime(day_to_predict)][['ID','total','date']].tail(5))\n",
        "    results['total']=results['Label']\n",
        "    results.drop('Label',axis=1,inplace=True)\n",
        "    print(df.shape)\n",
        "    df=pd.concat([df,results])\n",
        "    print(df.shape)\n",
        "    df.sort_values(['ID','date'],inplace=True)\n",
        "predictions=df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKmB0oK-9TEQ"
      },
      "outputs": [],
      "source": [
        "a=df[df['date']>='2020-01-30']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "webQoLhu9lhC"
      },
      "outputs": [],
      "source": [
        "a.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3DSRS1l-ATm"
      },
      "outputs": [],
      "source": [
        "b=aggregated[aggregated['date']>='2020-01-30']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BECbu3BM-HxR"
      },
      "outputs": [],
      "source": [
        "b.head(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbOyrQaA30if"
      },
      "outputs": [],
      "source": [
        "df_full_to_get_temporal_features.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7zRINjoblxg"
      },
      "outputs": [],
      "source": [
        "output_data.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_LOdTtBeHUl"
      },
      "outputs": [],
      "source": [
        "one_id=output_data[output_data['ID']==0].copy()\n",
        "# one_id=one_id[(one_id['date']>='2020-01-16') & (one_id['date']<='2020-01-22')]\n",
        "# one_id.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJpEL8LkieSr"
      },
      "outputs": [],
      "source": [
        "one_id.Label.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Xn9RMvby4o"
      },
      "outputs": [],
      "source": [
        "algo=output_data.pivot(\n",
        "    index='ID',\n",
        "    columns='date',\n",
        "    values='Label'\n",
        ")\n",
        "algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbwFj6b3sCQe"
      },
      "outputs": [],
      "source": [
        "def create_the_sum_between_dates(df,init_date:str,end_date:str):\n",
        "    return df[(df['date']>=init_date) & (df['date']<=end_date)].Label.sum()\n",
        "\n",
        "def create_the_two_rows_extra_per_id(df):\n",
        "    df_aux=pd.DataFrame()\n",
        "    df=df[['ID','Label','date']]\n",
        "    init_date_1='2020-01-01' #cambiar por febrero\n",
        "    end_date_1='2020-01-07'\n",
        "    init_date_2='2020-01-18'\n",
        "    end_date_2='2020-01-25'\n",
        "    for id in tqdm(df.ID.unique(),desc='features temporal variables per id'):\n",
        "            \n",
        "        df_per_id=df[df['ID']==id]\n",
        "        df_per_id['Semana_1']=create_the_sum_between_dates(df_per_id,init_date_1,end_date_1)\n",
        "        df_per_id['Semana_2']=create_the_sum_between_dates(df_per_id,init_date_2,end_date_2)\n",
        "            \n",
        "        df_per_id=df_per_id[['ID','Semana_1','Semana_2']].drop_duplicates()\n",
        "        df_aux=pd.concat([df_per_id,df_aux])    \n",
        "    return df_aux\n",
        "\n",
        "def create_pivot_table(df):\n",
        "    df=df.pivot(\n",
        "        index='ID',\n",
        "        columns='date',\n",
        "        values='Label'\n",
        "    )\n",
        "    # replace the Month with year \n",
        "    # df = df.rename(columns={\"date\":\"ID\"})\n",
        "\n",
        "    # drop first column\n",
        "    # df = df.iloc[1:].reset_index(drop=True)\n",
        "    # df.reset_index(inplace=True)\n",
        "    # df.drop('date',inplace=True,axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_output_df(df):\n",
        "    pivot_table=create_pivot_table(df)\n",
        "    values_per_week=create_the_two_rows_extra_per_id(df)\n",
        "    df=pd.merge(pivot_table,values_per_week,on='ID')\n",
        "    return df\n",
        "\n",
        "def create_outputfile(df,path_output='Cajamar_Universitat Politécnica de Valencia (UPV)_CanarIAs_1.txt'):\n",
        "    #Sin cabecera ni nombres de filas.\n",
        "    # Constará de 2.747 filas con 10 columnas cada fila:\n",
        "    # • ID: ordenado de forma ascendente\n",
        "    # • Dia_1: Predicción para el día 01/02/2020\n",
        "    # • Dia_2: Predicción para el día 02/02/2020\n",
        "    # • Dia_3: Predicción para el día 03/02/2020\n",
        "    # • Dia_4: Predicción para el día 04/02/2020\n",
        "    # • Dia_5: Predicción para el día 05/02/2020\n",
        "    # • Dia_6: Predicción para el día 06/02/2020\n",
        "    # • Dia_7: Predicción para el día 07/02/2020\n",
        "    # • Semana_1: Predicción para la semana del 01/02 al 07/02/2020, ambos inclusive\n",
        "    # • Semana_2: Predicción para la semana del 08/02 al 14/02/2020, ambos inclusive\n",
        "    #Separando campos con “|”, el valor de la predicción en litros, y los decimales con “.” (incluir solo 2 decimales).\n",
        "    # Ejemplo \"Fichero respuesta\" (primeras líneas)\n",
        "\n",
        "    df=create_output_df(df)\n",
        "    \n",
        "    df.to_csv(path_output,sep='|',index=False,header=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLeYLzUivgGJ"
      },
      "outputs": [],
      "source": [
        "df_output=create_outputfile(output_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0bKBX9B6zY5"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSQjoXmevjKl"
      },
      "source": [
        "check prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wck1rKFIvgdf"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf2p_EPhvt5t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr6y8j3vvssS"
      },
      "outputs": [],
      "source": [
        "target='Label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO_hHrabvko3"
      },
      "outputs": [],
      "source": [
        "path_predictions='/content/predictions_0_22_03_13_v2.csv'\n",
        "\n",
        "a=pd.read_csv(path_predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XemyL82BwQOq"
      },
      "outputs": [],
      "source": [
        "plot_results(a,a,stop=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQvR_SYd-q8q"
      },
      "outputs": [],
      "source": [
        "a['total']=150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st5G0CtMwdHw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(a['total'],a['Label'],squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpZp0A9h_DnQ"
      },
      "outputs": [],
      "source": [
        "a['error']=a['Label']-a['total']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH41GOLAHxZQ"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TkPsKJJHx7t"
      },
      "outputs": [],
      "source": [
        "y_true = [3, -0.5, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_s76jzTH9Cp"
      },
      "outputs": [],
      "source": [
        "errors=[]\n",
        "for i in range(0,len(y_true)):\n",
        "    error=y_true[i]-y_pred[i]\n",
        "    errors.append(error)\n",
        "errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6vUU_jDIANK"
      },
      "outputs": [],
      "source": [
        "0.5*0.5+0.5*0.5+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9aDJPk3IYRl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNNHGTASIetJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OouX9RRw4ZQ-"
      ],
      "name": "second_analisis para crear variables cajamar 2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}